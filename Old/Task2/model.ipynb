{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ivan\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 625/625 [00:00<00:00, 313kB/s]\n",
      "c:\\Users\\Ivan\\miniconda3\\envs\\tf\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ivan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading model.safetensors: 100%|██████████| 714M/714M [01:21<00:00, 8.74MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 2.06MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.3kB/s]\n",
      "c:\\Users\\Ivan\\miniconda3\\envs\\tf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(labels, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# Crea il DataLoader per l'addestramento\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mTensorDataset(input_ids, attention_masks, labels)\n\u001b[0;32m     61\u001b[0m dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     63\u001b[0m \u001b[39m# Addestramento del modello\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ivan\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\utils\\data\\dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[1;34m(self, *tensors)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mtensors: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(tensors[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m tensor\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m tensors), \u001b[39m\"\u001b[39m\u001b[39mSize mismatch between tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors \u001b[39m=\u001b[39m tensors\n",
      "\u001b[1;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "# Caricamento dei dati dal file JSON\n",
    "with open('../RemovedStopWordData/ate_absita_training_extracted.json', 'r') as file:\n",
    "    data_train = json.load(file)\n",
    "\n",
    "with open('../RemovedStopWordData/ate_absita_gold_extracted.json', 'r') as file:\n",
    "    data_test = json.load(file)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-uncased\")\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"dbmdz/bert-base-italian-xxl-uncased\")\n",
    "\n",
    "train_sentences = data_train['aspects']\n",
    "train_labels = data_train['polarities']\n",
    "\n",
    "\n",
    "\n",
    "train_sentences_new = []\n",
    "train_labels_new = []\n",
    "for index, (aspects, labels) in enumerate(zip(train_sentences,train_labels)):\n",
    "    for jndex, (aspect, label) in enumerate(zip(aspects,labels)):\n",
    "        train_sentences_new.append(aspect)\n",
    "        train_labels_new.append(label)\n",
    "\n",
    "print(len(train_sentences_new))\n",
    "print(len(train_labels_new))\n",
    "    \n",
    "\n",
    "# Tokenizzazione dei dati di addestramento\n",
    "train_encodings = tokenizer.batch_encode_plus(train_sentences_new, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# Creazione del dataset TensorFlow per l'addestramento\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'input_ids': tf.convert_to_tensor(train_encodings['input_ids']),\n",
    "        'attention_mask': tf.convert_to_tensor(train_encodings['attention_mask'])\n",
    "    },\n",
    "    tf.convert_to_tensor(train_labels_new)\n",
    "))\n",
    "\n",
    "# Addestramento del modello\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "model.fit(train_dataset.shuffle(1000).batch(16), epochs=4)\n",
    "\n",
    "test_aspect_texts = data_test['aspects']\n",
    "test_labels = data_test['polarities']\n",
    "\n",
    "train_sentences_new = []\n",
    "train_labels_new = []\n",
    "for index, (aspects, labels) in enumerate(zip(test_aspect_texts,test_labels)):\n",
    "    for jndex, (aspect, label) in enumerate(zip(aspects,labels)):\n",
    "        train_sentences_new.append(aspect)\n",
    "        train_labels_new.append(label)\n",
    "\n",
    "# Esempio di frasi di input per le predizioni\n",
    "#input_sentences = [\"Il design è molto elegante.\", \"Il prodotto non funziona correttamente.\"]\n",
    "\n",
    "# Tokenizzazione dei dati di input per le predizioni\n",
    "input_encodings = tokenizer.batch_encode_plus(train_sentences_new, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "\n",
    "# Creazione del dataset TensorFlow per le predizioni\n",
    "input_dataset = tf.data.Dataset.from_tensor_slices(dict(input_encodings)).batch(16)\n",
    "\n",
    "# Predizioni\n",
    "predictions = model.predict(input_dataset)\n",
    "\n",
    "\n",
    "\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
