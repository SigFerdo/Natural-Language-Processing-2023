{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ferdo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Rimuovi la punteggiatura\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Trasforma tutte le lettere in minuscolo\n",
    "    text = text.lower()\n",
    "    # Rimuovi le parole vuote\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Pre-processa i dati\n",
    "    for item in data:\n",
    "        item['sentence'] = preprocess_text(item['sentence'])\n",
    "    # Salva i dati pre-processati in un nuovo file\n",
    "    base_name = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, base_name)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Pre-processa i dati del training set, del test set e del validation set\n",
    "remove_stopwords('Dataset/ate_absita_training.ndjson', 'RemovedStopWordData')\n",
    "remove_stopwords('Dataset/ate_absita_test.ndjson', 'RemovedStopWordData')\n",
    "remove_stopwords('Dataset/ate_absita_gold.ndjson', 'RemovedStopWordData')\n",
    "remove_stopwords('Dataset/ate_absita_dev.ndjson', 'RemovedStopWordData')\n",
    "\n",
    "def extract_columns(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Estrai le colonne di interesse\n",
    "    sentences = [item['sentence'] for item in data]\n",
    "    scores = [item['score'] for item in data]\n",
    "    # Crea un dizionario per contenere le colonne estratte\n",
    "    extracted_data = {'sentences': sentences, 'scores': scores}\n",
    "    # Salva il dizionario in un nuovo file JSON\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    output_path = os.path.join(output_dir, f'{base_name}_extracted.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(extracted_data, f)\n",
    "\n",
    "\n",
    "#estrazione delle colonne rilevanti dal dataset con la rimozione delle stopwords\n",
    "extract_columns('RemovedStopWordData/ate_absita_training.ndjson', 'RemovedStopWordData')\n",
    "extract_columns('RemovedStopWordData/ate_absita_gold.ndjson', 'RemovedStopWordData')\n",
    "extract_columns('RemovedStopWordData/ate_absita_dev.ndjson', 'RemovedStopWordData')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ferdo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ferdo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Rimuovi la punteggiatura\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Trasforma tutte le lettere in minuscolo\n",
    "    text = text.lower()\n",
    "    # Rimuovi le parole vuote\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Effettua la lemmatizzazione delle parole\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def preprocess_file(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Pre-processa i dati\n",
    "    for item in data:\n",
    "        item['sentence'] = preprocess_text(item['sentence'])\n",
    "    # Salva i dati pre-processati in un nuovo file\n",
    "    base_name = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, base_name)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Pre-processa i dati del training set, del test set e del validation set con lemmatizzazione\n",
    "preprocess_file('Dataset/ate_absita_training.ndjson', 'LemmatizedData')\n",
    "preprocess_file('Dataset/ate_absita_test.ndjson', 'LemmatizedData')\n",
    "preprocess_file('Dataset/ate_absita_gold.ndjson', 'LemmatizedData')\n",
    "preprocess_file('Dataset/ate_absita_dev.ndjson', 'LemmatizedData')\n",
    "\n",
    "def extract_columns(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Estrai le colonne di interesse\n",
    "    sentences = [item['sentence'] for item in data]\n",
    "    scores = [item['score'] for item in data]\n",
    "    # Crea un dizionario per contenere le colonne estratte\n",
    "    extracted_data = {'sentences': sentences, 'scores': scores}\n",
    "    # Salva il dizionario in un nuovo file JSON\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    output_path = os.path.join(output_dir, f'{base_name}_extracted.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(extracted_data, f)\n",
    "\n",
    "extract_columns('LemmatizedData/ate_absita_training.ndjson', 'LemmatizedData')\n",
    "extract_columns('LemmatizedData/ate_absita_gold.ndjson', 'LemmatizedData')\n",
    "extract_columns('LemmatizedData/ate_absita_dev.ndjson', 'LemmatizedData')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmatizzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Rimuovi la punteggiatura\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Trasforma tutte le lettere in minuscolo\n",
    "    text = text.lower()\n",
    "    # Rimuovi le parole vuote\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Effettua la stemmatizzazione delle parole\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def preprocess_file(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Pre-processa i dati\n",
    "    for item in data:\n",
    "        item['sentence'] = preprocess_text(item['sentence'])\n",
    "    # Salva i dati pre-processati in un nuovo file\n",
    "    base_name = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, base_name)\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Pre-processa i dati del training set, del test set e del validation set con stemmatizzazione\n",
    "preprocess_file('Dataset/ate_absita_training.ndjson', 'StemmedData')\n",
    "preprocess_file('Dataset/ate_absita_test.ndjson', 'StemmedData')\n",
    "preprocess_file('Dataset/ate_absita_gold.ndjson', 'StemmedData')\n",
    "preprocess_file('Dataset/ate_absita_dev.ndjson', 'StemmedData')\n",
    "\n",
    "def extract_columns(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Estrai le colonne di interesse\n",
    "    sentences = [item['sentence'] for item in data]\n",
    "    scores = [item['score'] for item in data]\n",
    "    # Crea un dizionario per contenere le colonne estratte\n",
    "    extracted_data = {'sentences': sentences, 'scores': scores}\n",
    "    # Salva il dizionario in un nuovo file JSON\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    output_path = os.path.join(output_dir, f'{base_name}_extracted.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(extracted_data, f)\n",
    "\n",
    "extract_columns('StemmedData/ate_absita_training.ndjson', 'StemmedData')\n",
    "extract_columns('StemmedData/ate_absita_gold.ndjson', 'StemmedData')\n",
    "extract_columns('StemmedData/ate_absita_gold.ndjson', 'StemmedData')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(file_path, output_dir):\n",
    "    # Crea la cartella di output se non esiste\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    # Leggi i dati dal file\n",
    "    data = []\n",
    "    with open(file_path, 'r',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    # Estrai le colonne di interesse\n",
    "    sentences = [item['sentence'] for item in data]\n",
    "    scores = [item['score'] for item in data]\n",
    "    # Crea un dizionario per contenere le colonne estratte\n",
    "    extracted_data = {'sentences': sentences, 'scores': scores}\n",
    "    # Salva il dizionario in un nuovo file JSON\n",
    "    base_name, _ = os.path.splitext(os.path.basename(file_path))\n",
    "    output_path = os.path.join(output_dir, f'{base_name}_extracted.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(extracted_data, f)\n",
    "\n",
    "# Estrazione delle colonne rilevanti dal dataset originale\n",
    "extract_columns('Dataset/ate_absita_training.ndjson', 'DefaultData')\n",
    "extract_columns('Dataset/ate_absita_gold.ndjson', 'DefaultData')\n",
    "extract_columns('Dataset/ate_absita_dev.ndjson', 'DefaultData')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
